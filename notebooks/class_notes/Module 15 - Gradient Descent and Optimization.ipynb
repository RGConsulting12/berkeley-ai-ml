{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be8ddee",
   "metadata": {},
   "source": [
    "# Module 15: Gradient Descent and Optimization\n",
    "\n",
    "## Learning Outcomes\n",
    "- • Compute the approximate value of a function using its derivative.\n",
    "- • Identify the basics of gradient descent and the impact of learning rate on convergence.\n",
    "- • Compute x and/or y for each iteration of gradient descent.\n",
    "- • Optimize a single-parameter linear regression model from scratch.\n",
    "- • Recognize convex one-dimensional and two-dimensional functions.\n",
    "- • Compute the gradient of a two-dimensional function.\n",
    "- • Measure the impurity of a decision tree using entropy.\n",
    "- • Use gradient descent to optimize a nonlinear two-dimensional regression model.\n",
    "- • Use stochastic gradient descent to optimize a nonlinear two-dimensional regression model.\n",
    "- • Compare the convergence behavior of gradient descent with stochastic gradient descent.\n",
    "- • Articulate the impact of stochastic gradient on bias and variance.\n",
    "\n",
    "## Key Activities\n",
    "- • Videos 15.1–15.15\n",
    "- • Required Knowledge Checks 15.1–15.6\n",
    "- • Required Codio Assignments 15.1–15.4\n",
    "- • Self-Study Colab Activities 15.1–15.4\n",
    "\n",
    "## Notes\n",
    "\n",
    "## Example Code\n",
    "\n",
    "## Summary\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
